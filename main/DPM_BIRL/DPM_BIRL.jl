module DPMBIRL

export DPMBIRL, generate_gridworld, generate_trajectories

using POMDPs
using POMDPModels
using Distributions
using POMDPToolbox

immutable Globals
    n_states::Int64
    n_actions::Int64
    n_features::Int64
    n_trajectories::Int64
    actions_i::Array{Int64}
    Œ≤::Float64
    Œ≥::Float64
    P‚Çê::Array{Array{Float64,2},1}
    œá::Array{MDPHistory}
    œï::Array{Float64,2}
end

include("../reward.jl")
include("../cluster.jl")
include("../utilities/gridworld.jl")
include("../utilities/policy.jl")
include("../utilities/general.jl")
include("../utilities/trajectory.jl")


function log_evd!(log, mdp, Œ∏s, ground_truth)
    tmp_mdp = copy(mdp)
    tmp_mdp.reward_values = ground_truth[:reward]
    œÄ·µ£ = solve_mdp(mdp, Œ∏)
    v·µ£ = policy_evaluation(tmp_mdp, œÄ·µ£)
    push!(log, norm(ground_truth[:v] - v·µ£))
end


"""
    (proportional) Likelihood function for a single state action
    Normally should have normalisiation, but not important when calculating ‚àáùìõ
"""
state_action_lh(œÄ·µ¶, s,a) = œÄ·µ¶[s,a]

"""
    Executes a maximum likelihood inverse reinforcement learning
    mdp:            the problem
    œï:              operator to features space
    trajectories:   experts' trajectories
    iterations:     number of iterations to run for
    Œ±:              learning rate
    Œ≤:              confidence parameter
    Œ∫:              concentration parameter for DPM
"""
function DPM_BIRL(mdp, œï, œá, iterations; Œ±=0.1, Œ∫=1., Œ≤=0.5, ground_truth = nothing, verbose=true, update=:ML)

    verbose ? println("Using $(update) update") : nothing

    œÑ = sqrt(2*Œ±)

    Œ≥ = mdp.discount_factor
    states = ordered_states(mdp)

    n_states  = size(states,1)-1
    n_actions = size( actions(mdp),1 )
    n_features = size(œï,2)
    n_trajectories = size(œá,1)

    EVD = []

    # Precpmputes transition matrix for all actions
    # (independent of features)
    P‚Çê = a2transition.(mdp,actions(mdp))
    actions_i = action_index.(mdp, actions(mdp))

    const glb = Globals(n_states, n_actions, n_features, n_trajectories, actions_i, Œ≤, Œ≥, P‚Çê, œá, œï)

    #### Initialisation ####
    # Initialise clusters
    # K = n_trajectories
    K = 1
    # assignements    = collect(1:n_trajectories)
    # assignements    = rand(1:K, n_trajectories)
    assignements = fill(1,n_trajectories)

    N = map(x->sum(assignements .== x), 1:K)


    # Prepare reward functions
    Œ∏s = [sample(RewardFunction, n_features) for i in 1:K]
    for (k,Œ∏) in enumerate(Œ∏s)
        assigned2cluster = (assignements .== k)
        œá‚Çñ = œá[assigned2cluster]
        update_reward!(Œ∏, mdp, œá‚Çñ, glb)
    end

    ùìõ_traj = ones(n_trajectories)*1e-5
    c      = Clusters(K, assignements, N, ùìõ_traj, Œ∏s)

    # update_clusters!(c, mdp, Œ∫, glb)

    log = Dict(:assignements => [], :EVDs => [], :likelihoods => [], :rewards => [])

    for t in 1:iterations
        tic()

        # update_clusters!(c, mdp, Œ∫, glb)

        for (k, Œ∏) in enumerate(c.rewards)
            # Find potential new reward
            if update == :langevin_rand
                Œ∏‚Åª = Œ∏ + Œ± * ‚àáùìõ + Œ± * rand(Normal(0,1), n_features)
            else
                Œ∏‚Åª = Œ∏ + Œ± * Œ∏.‚àáùìõ
            end

            # Solve everything for potential new reward
            œÄ‚Åª  = solve_mdp(mdp, Œ∏‚Åª)
            œÄ·µ¶‚Åª = calœÄ·µ¶(mdp, œÄ‚Åª.qmat, glb)
            invT‚Åª = calInvTransition(mdp, œÄ·µ¶‚Åª, Œ≥)

            # Calculate likelihood and gradient
            assigned2cluster = (c.assignements .== k)
            œá‚Çñ = œá[assigned2cluster]
            ùìõ‚Åª, ‚àáùìõ‚Åª = cal‚àáùìõ(mdp, invT‚Åª, œÄ·µ¶‚Åª,  œá‚Çñ, glb)

            # Do the update
            if update == :ML
                # We simply follow the gradient
                Œ∏.values, Œ∏.ùìõ, Œ∏.‚àáùìõ, Œ∏.invT, Œ∏.œÄ, Œ∏.œÄ·µ¶ = Œ∏‚Åª.values, ùìõ‚Åª, ‚àáùìõ‚Åª, invT‚Åª, œÄ‚Åª, œÄ·µ¶‚Åª
            elseif update == :langevin || update == :langevin_rand
                # Use result from Choi
                Œ∏.ùìõ += sum(pdf.(Normal(0,1), Œ∏.values))
                ùìõ‚Åª += sum(pdf.(Normal(0,1), Œ∏‚Åª.values))
                p =  ùìõ‚Åª / Œ∏.ùìõ * proposal_distribution(Œ∏‚Åª, Œ∏, ‚àáùìõ‚Åª, œÑ) / proposal_distribution(Œ∏, Œ∏‚Åª, Œ∏.‚àáùìõ, œÑ)
                if rand() > p
                    Œ∏.values, Œ∏.ùìõ, Œ∏.‚àáùìõ, Œ∏.invT, Œ∏.œÄ, Œ∏.œÄ·µ¶ = Œ∏‚Åª.values, ùìõ‚Åª, ‚àáùìõ‚Åª, invT‚Åª, œÄ‚Åª, œÄ·µ¶‚Åª
                end
            end
        end

        elapsed = toq()

        # Log EVD
        if verbose
            println("Iteration took $elapsed seconds")
        end
        push!(log[:assignements], copy(c.N))
        push!(log[:likelihoods], map(x->x.ùìõ, c.rewards))
        push!(log[:rewards], c.rewards)

        if ground_truth !== nothing
            # EVDs = []
            # for Œ∏ in c.rewards
            #     mdp·µ£ = reward_mdp(mdp, Œ∏)
            #     v·µ£ = policy_evaluation(mdp·µ£, Œ∏.œÄ)
            #     push!(EVDs, norm(v-v·µ£))
            # end
            # push!(log[:EVDs], EVDs)
            #
            # mdp·µ£ = reward_mdp(mdp, Œ∏s[1])
            log_evd!(log[:EVDs], mdp, Œ∏s[1], ground_truth)
        end
    end

    # Log EVD
    # if verbose && ground_policy !== nothing
    #     # Need to change this to account for features
    #     œÄ·µ£ = solve_mdp(mdp, c.rewards[1])
    #     v·µ£ = policy_evaluation(mdp, œÄ·µ£)
    #     push!(EVD, norm(v-v·µ£))
    #     println("Final EVD: $(EVD[end])")
    # end

    c, EVD, log
end



# End module
end
