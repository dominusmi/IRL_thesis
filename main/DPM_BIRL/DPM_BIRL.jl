module DPMBIRL

export DPMBIRL, generate_gridworld, generate_trajectories

using POMDPs
using POMDPModels
using Distributions
using POMDPToolbox

immutable Globals
    n_states::Int64
    n_actions::Int64
    n_features::Int64
    n_trajectories::Int64
    actions_i::Array{Int64}
    β::Float64
    γ::Float64
    Pₐ::Array{Array{Float64,2},1}
    χ::Array{MDPHistory}
    ϕ::Array{Float64,2}
end

include("../reward.jl")
include("../cluster.jl")
include("../utilities/gridworld.jl")
include("../utilities/policy.jl")
include("../utilities/general.jl")
include("../utilities/trajectory.jl")

# Logs a EVD matrix, where the rows are the ground-truths,
# and the columns are the EVD with respect to the reward functions
function log_evd!(_log, mdp, θs, ground_truth)
    # EVD is how well does a policy generated by r' behave on an MDP
    # with the true reward function r
    vs = ground_truth[:vs]
    EVD_matrix = zeros(size(vs,1), size(θs,1))
    for (i,v) in enumerate(vs)
        # Make the MDP with the real reward values of the ith agent
        tmp_mdp = copy(mdp)
        tmp_mdp.reward_values = ground_truth[:rewards][i]
        for (j,θ) in enumerate(θs)
            # Solve the MDP with our reward function and get the optimal π
            πᵣ = solve_mdp(mdp, θ)
            # Check how well does π work w.r.t. the optimal value function
            vᵣ = policy_evaluation(tmp_mdp, πᵣ)
            EVD_matrix[i,j] = norm(v - vᵣ,1)
        end
    end
    push!(_log, EVD_matrix)
end


"""
    (proportional) Likelihood function for a single state action
    Normally should have normalisiation, but not important when calculating ∇𝓛
"""
state_action_lh(πᵦ, s,a) = πᵦ[s,a]

"""
    Executes a maximum likelihood inverse reinforcement learning
    mdp:            the problem
    ϕ:              operator to features space
    trajectories:   experts' trajectories
    iterations:     number of iterations to run for
    α:              learning rate
    β:              confidence parameter
    κ:              concentration parameter for DPM
    burn_in:        number of iterations not to record (at the beginning)
"""
function DPM_BIRL(mdp, ϕ, χ, iterations; α=0.1, κ=1., β=0.5, ground_truth = nothing, verbose=true, update=:ML, burn_in=5)

    verbose ? println("Using $(update) update") : nothing

    τ = sqrt(2*α)

    γ = mdp.discount_factor
    states = ordered_states(mdp)

    n_states  = size(states,1)-1
    n_actions = size( actions(mdp),1 )
    n_features = size(ϕ,2)
    n_trajectories = size(χ,1)

    # Precpmputes transition matrix for all actions
    # (independent of features)
    Pₐ = a2transition.(mdp,actions(mdp))
    actions_i = action_index.(mdp, actions(mdp))

    const glb = Globals(n_states, n_actions, n_features, n_trajectories, actions_i, β, γ, Pₐ, χ, ϕ)

    #### Initialisation ####
    # Initialise clusters
    # K = n_trajectories
    K = 5
    # assignements    = collect(1:n_trajectories)
    assignements    = rand(1:K, n_trajectories)
    # assignements = fill(1,n_trajectories)

    N = map(x->sum(assignements .== x), 1:K)


    # Prepare reward functions
    θs = [sample(RewardFunction, n_features) for i in 1:K]
    for (k,θ) in enumerate(θs)
        assigned2cluster = (assignements .== k)
        χₖ = χ[assigned2cluster]
        update_reward!(θ, mdp, χₖ, glb)
    end

    𝓛_traj = ones(n_trajectories)*1e-5
    c      = Clusters(K, assignements, N, 𝓛_traj, θs)

    update_clusters!(c, mdp, κ, glb)

    _log = Dict(:assignements => [], :EVDs => [], :likelihoods => [], :rewards => [], :clusters=>[])

    for t in 1:iterations
        tic()

        updated_clusters_id = update_clusters!(c, mdp, κ, glb)
        verbose ? println("Clusters changed: $(length(updated_clusters_id)) of $(c.K)") : nothing

        for (k, θ) in enumerate(c.rewards)
            # Get the clusters' trajectories
            assigned2cluster = (c.assignements .== k)
            χₖ = χ[assigned2cluster]

            # Update likelihood and gradient to current cluster
            # TODO: only do this step if cluster changed
            if k in updated_clusters_id
                θ.𝓛, θ.∇𝓛 = cal∇𝓛(mdp, θ.invT, θ.πᵦ,  χₖ, glb)
            end

            # Find potential new reward
            if update == :langevin_rand
                ϵ = α * rand(Normal(0,1), n_features)
                indeces = rand(n_features) .< 0.2
                ϵ[indeces] = 0.0
                θ⁻ = θ + α * θ.∇𝓛 + ϵ
            else
                θ⁻ = θ + α * θ.∇𝓛
            end

            # Solve everything for potential new reward
            π⁻  = solve_mdp(mdp, θ⁻)
            πᵦ⁻ = calπᵦ(mdp, π⁻.qmat, glb)
            invT⁻ = calInvTransition(mdp, πᵦ⁻, γ)

            # Calculate likelihood and gradient for new reward
            𝓛⁻, ∇𝓛⁻ = cal∇𝓛(mdp, invT⁻, πᵦ⁻,  χₖ, glb)


            # Do the update
            if update == :ML
                # We simply follow the gradient
                θ.values, θ.𝓛, θ.∇𝓛, θ.invT, θ.π, θ.πᵦ = θ⁻.values, 𝓛⁻, ∇𝓛⁻, invT⁻, π⁻, πᵦ⁻
            elseif update == :langevin || update == :langevin_rand
                # Use result from Choi
                θ.𝓛 += sum(logpdf.(Normal(0,1), θ.values))
                𝓛⁻ += sum(logpdf.(Normal(0,1), θ⁻.values))
                # logpd⁻ = Base.log(proposal_distribution(θ⁻, θ, ∇𝓛⁻, τ))
                # logpd  = Base.log(proposal_distribution(θ, θ⁻, θ.∇𝓛, τ))
                logpd⁻ = proposal_distribution(θ⁻, θ, ∇𝓛⁻, τ)
                logpd = proposal_distribution(θ, θ⁻, θ.∇𝓛, τ)

                # print("($(@sprintf("%.2f", θ.𝓛)), $(@sprintf("%.2f", 𝓛⁻)), $(@sprintf("%.2f", logpd)), $(@sprintf("%.2f", logpd⁻)))")

                p =  𝓛⁻ / θ.𝓛 * logpd⁻ / logpd
                # p = exp( 𝓛⁻ + logpd⁻ - θ.𝓛 - logpd)
                print("$p, ")
                if rand() < p
                    θ.values, θ.𝓛, θ.∇𝓛, θ.invT, θ.π, θ.πᵦ = θ⁻.values, 𝓛⁻, ∇𝓛⁻, invT⁻, π⁻, πᵦ⁻
                end
            end
        end

        elapsed = toq()

        # Log EVD
        verbose ? println("Iteration number $t took $elapsed seconds") : nothing
        if t > burn_in
            push!(_log[:assignements], copy(c.N))
            push!(_log[:likelihoods], map(x->x.𝓛, c.rewards))
            push!(_log[:rewards], copy(c.rewards))
            push!(_log[:clusters], copy(c))

            if ground_truth !== nothing
                log_evd!(_log[:EVDs], mdp, c.rewards, ground_truth)
                verbose ? show(_log[:EVDs][end]) : nothing
            end
        end
    end

    c, _log
end



# End module
end
