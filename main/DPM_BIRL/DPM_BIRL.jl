include("../reward.jl")
include("../utilities/gridworld.jl")
include("../utilities/policy.jl")
include("../utilities/general.jl")


"""
    (proportional) Likelihood function for a single state action
    Normally should have normalisiation, but not important when calculating ‚àáùìõ
"""
state_action_lh(œÄ·µ¶, s,a) = œÄ·µ¶[s,a]

"""
    Executes a maximum likelihood inverse reinforcement learning
    mdp:            the problem
    œï:              operator to features space
    trajectories:   experts' trajectories
    iterations:     number of iterations to run for
    Œ±:              learning rate
    Œ≤:              confidence parameter
"""
function MLIRL(mdp, œï, trajectories, iterations; Œ±=0.1, Œ≤=0.5, ground_policy = nothing, verbose=true)

    Œ∏ = sample(RewardFunction, size(œï,2))
    Œ≥ = mdp.discount_factor
    states = ordered_states(mdp)

    n_states  = size(states,1)-1
    n_actions = size( actions(mdp),1 )
    n_features = size(œï,2)

    EVD = []

    if verbose && ground_policy !== nothing
        v = policy_evaluation(mdp, ground_policy)
    end

    for t in 1:iterations
        tic()
        # œÄ, Q·µ£   = calQ·µ£(mdp, Œ∏)
        œÄ  = solve_mdp(mdp, Œ∏)
        œÄ·µ¶ = calœÄ·µ¶(mdp, œÄ.qmat, Œ≤)

        # Log EVD
        if verbose && ground_policy !== nothing
            tic()
            # Need to change this to account for features
            œÄ·µ£ = solve_mdp(mdp, Œ∏)
            v·µ£ = policy_evaluation(mdp, œÄ·µ£)
            push!(EVD, norm(v-v·µ£))
            elapsed = toq()
        end


        invT = calInvTransition(mdp, œÄ·µ¶, Œ≥)
        actions_i = action_index.(mdp, actions(mdp))
        dQ = zeros(n_states, n_actions)
        ‚àáùìõ = SharedArray{Float64,1}(zeros(n_features))

        # Precpmputes transition matrix for all actions
        # (independent of features)
        P‚Çê = a2transition.(mdp,actions(mdp))


        # Calculates gradient per feature
        for k in 1:n_features
            dQ‚Çñ = zeros( n_states, n_actions )
            caldQ‚Çñ!(dQ‚Çñ, mdp, œï, invT, P‚Çê, œÄ·µ¶, k)
            # Calculates total gradient over trajectories
            for (m,trajectory) in enumerate(trajectories)
                for (h,state) in enumerate(trajectory.state_hist[1:end-1])
                    s‚Çï = state_index(mdp, state)
                    a‚Çï = action_index(mdp, trajectory.action_hist[h])

                    dl_dŒ∏‚Çñ = Œ≤ * ( dQ‚Çñ[s‚Çï,a‚Çï] - sum( [ state_action_lh(œÄ·µ¶,s‚Çï,ai‚Åª) * dQ‚Çñ[s‚Çï,ai‚Åª] for ai‚Åª in actions_i ] ) )
                    ‚àáùìõ[k] += dl_dŒ∏‚Çñ
                end
            end
        end
        Œ∏ += Œ± * ‚àáùìõ
        # Œ∏.values /= maximum(Œ∏.values)
        elapsed = toq()

        verbose ? println("Iteration took $elapsed seconds") : nothing
    end

    # Log EVD
    if verbose && ground_policy !== nothing
        # Need to change this to account for features
        œÄ·µ£ = solve_mdp(mdp, Œ∏)
        v·µ£ = policy_evaluation(mdp, œÄ·µ£)
        push!(EVD, norm(v-v·µ£))
        println("Final EVD: $(EVD[end])")
    end

    Œ∏, EVD
end
