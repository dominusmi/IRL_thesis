module DPMBIRL

export DPMBIRL, generate_gridworld, generate_trajectories

using POMDPs
using POMDPModels
using Distributions
using POMDPToolbox

immutable Globals
    n_states::Int64
    n_actions::Int64
    n_features::Int64
    n_trajectories::Int64
    actions_i::Array{Int64}
    Œ≤::Float64
    Œ≥::Float64
    P‚Çê::Array{Array{Float64,2},1}
    œá::Array{MDPHistory}
    œï::Array{Float64,2}
end

include("../reward.jl")
include("../cluster.jl")
include("../utilities/gridworld.jl")
include("../utilities/policy.jl")
include("../utilities/general.jl")
include("../utilities/trajectory.jl")



"""
    (proportional) Likelihood function for a single state action
    Normally should have normalisiation, but not important when calculating ‚àáùìõ
"""
state_action_lh(œÄ·µ¶, s,a) = œÄ·µ¶[s,a]

"""
    Executes a maximum likelihood inverse reinforcement learning
    mdp:            the problem
    œï:              operator to features space
    trajectories:   experts' trajectories
    iterations:     number of iterations to run for
    Œ±:              learning rate
    Œ≤:              confidence parameter
    Œ∫:              concentration parameter for DPM
"""
function DPM_BIRL(mdp, œï, œá, iterations; Œ±=0.1, Œ∫=1., Œ≤=0.5, ground_policy = nothing, verbose=true, update=:ML)

    verbose ? println("Using $(update) update") : nothing

    œÑ = sqrt(2*Œ±)

    Œ≥ = mdp.discount_factor
    states = ordered_states(mdp)

    n_states  = size(states,1)-1
    n_actions = size( actions(mdp),1 )
    n_features = size(œï,2)
    n_trajectories = size(œá,1)

    EVD = []

    if verbose && ground_policy !== nothing
        v = policy_evaluation(mdp, ground_policy)
    end

    # Precpmputes transition matrix for all actions
    # (independent of features)
    P‚Çê = a2transition.(mdp,actions(mdp))
    actions_i = action_index.(mdp, actions(mdp))

    const glb = Globals(n_states, n_actions, n_features, n_trajectories, actions_i, Œ≤, Œ≥, P‚Çê, œá, œï)

    #### Initialisation ####
    # Initialise clusters
    # K = n_trajectories
    K = 1
    assignements    = collect(1:n_trajectories)
    # assignements    = rand(1:K, n_trajectories)
    assignements = fill(1,n_trajectories)
    N               = map(x->sum(assignements .== x), 1:K)


    # Prepare reward functions
    Œ∏s = [sample(RewardFunction, n_features) for i in 1:K]
    for (k,Œ∏) in enumerate(Œ∏s)
    #     # Solve mdp with current reward
    #     Œ∏.œÄ  = solve_mdp(mdp, Œ∏)
    #     # Find Boltzmann policy
    #     Œ∏.œÄ·µ¶ = calœÄ·µ¶(mdp, Œ∏.œÄ.qmat, Œ≤)
    #
    #     # Prepare variables for gradient
    #     Œ∏.invT = calInvTransition(mdp, Œ∏.œÄ·µ¶, Œ≥)
    #     # Calculates value and gradient of trajectory likelihood
    #     Œ∏.ùìõ, Œ∏.‚àáùìõ = cal‚àáùìõ(mdp, œï, Œ∏.invT, P‚Çê, Œ∏.œÄ·µ¶, Œ≤, œá, n_states, n_actions, n_features, actions_i)
        assigned2cluster = (assignements .== k)
        œá‚Çñ = œá[assigned2cluster]
        update_reward!(Œ∏, mdp, œá‚Çñ, glb)
    end

    ùìõ_traj          = ones(n_trajectories)*1e-5
    c               = Clusters(K, assignements, N, ùìõ_traj, Œ∏s)

    update_clusters!(c, mdp, Œ∫, glb)

    log = Dict(:assignements => [], :EVDs => [], :likelihoods => [], :rewards => [])

    for t in 1:iterations
        tic()

        update_clusters!(c, mdp, Œ∫, glb)

        for (k, Œ∏) in enumerate(c.rewards)
            # Find potential new reward
            if update == :langevin_rand
                Œ∏‚Åª = Œ∏ + Œ± * ‚àáùìõ + Œ± * rand(Normal(0,1), n_features)
            else
                Œ∏‚Åª = Œ∏ + Œ± * Œ∏.‚àáùìõ
            end

            # Solve everything for potential new reward
            œÄ‚Åª  = solve_mdp(mdp, Œ∏‚Åª)
            œÄ·µ¶‚Åª = calœÄ·µ¶(mdp, œÄ‚Åª.qmat, glb)
            invT‚Åª = calInvTransition(mdp, œÄ·µ¶‚Åª, Œ≥)

            # Calculate likelihood and gradient
            assigned2cluster = (c.assignements .== k)
            œá‚Çñ = œá[assigned2cluster]
            ùìõ‚Åª, ‚àáùìõ‚Åª = cal‚àáùìõ(mdp, invT‚Åª, œÄ·µ¶‚Åª,  œá‚Çñ, glb)

            # Do the update
            if update == :ML
                # We simply follow the gradient
                Œ∏.values, Œ∏.ùìõ, Œ∏.‚àáùìõ, Œ∏.invT, Œ∏.œÄ, Œ∏.œÄ·µ¶ = Œ∏‚Åª.values, ùìõ‚Åª, ‚àáùìõ‚Åª, invT‚Åª, œÄ‚Åª, œÄ·µ¶‚Åª
            elseif update == :langevin || update == :langevin_rand
                # Use result from Choi
                ùìõ += sum(pdf.(Normal(0,1), Œ∏.values))
                ùìõ‚Åª += sum(pdf.(Normal(0,1), Œ∏‚Åª.values))
                p =  ùìõ‚Åª / Œ∏.ùìõ * proposal_distribution(Œ∏‚Åª, Œ∏, ‚àáùìõ‚Åª, œÑ) / proposal_distribution(Œ∏, Œ∏‚Åª, ‚àáùìõ, œÑ)
                @show p
                if rand() > p
                    Œ∏.values, Œ∏.ùìõ, Œ∏.‚àáùìõ, Œ∏.invT, Œ∏.œÄ, Œ∏.œÄ·µ¶ = Œ∏‚Åª.values, ùìõ‚Åª, ‚àáùìõ‚Åª, invT‚Åª, œÄ‚Åª, œÄ·µ¶‚Åª
                end
            end
        end

        elapsed = toq()

        # Log EVD
        if verbose
            println("Iteration took $elapsed seconds")
            push!(log[:assignements], copy(c.N))
            push!(log[:likelihoods], map(x->x.ùìõ, c.rewards))
            push!(log[:rewards], c.rewards)

            if ground_policy !== nothing
                EVDs = []
                for Œ∏ in c.rewards
                    v·µ£ = policy_evaluation(mdp, Œ∏.œÄ)
                    push!(EVDs, norm(v-v·µ£))
                end
                push!(log[:EVDs], EVDs)
                v·µ£ = policy_evaluation(mdp, Œ∏s[1].œÄ)
                push!(EVD, norm(v-v·µ£))
            end
        end
    end

    # Log EVD
    if verbose && ground_policy !== nothing
        # Need to change this to account for features
        œÄ·µ£ = solve_mdp(mdp, c.rewards[1])
        v·µ£ = policy_evaluation(mdp, œÄ·µ£)
        push!(EVD, norm(v-v·µ£))
        println("Final EVD: $(EVD[end])")
    end

    c, EVD, log
end



# End module
end
