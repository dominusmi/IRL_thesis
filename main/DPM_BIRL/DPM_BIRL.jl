module DPMBIRL

export DPMBIRL, generate_gridworld, generate_trajectories

using POMDPs
using POMDPModels
using Distributions
using POMDPToolbox

immutable Globals
    n_states::Int64
    n_actions::Int64
    n_features::Int64
    n_trajectories::Int64
    actions_i::Array{Int64}
    Î²::Float64
    Î³::Float64
    Pâ‚::Array{Array{Float64,2},1}
    Ï‡::Array{MDPHistory}
    Ï•::Array{Float64,2}
end

include("../reward.jl")
include("../cluster.jl")
include("../utilities/gridworld.jl")
include("../utilities/policy.jl")
include("../utilities/general.jl")
include("../utilities/trajectory.jl")

# Logs a EVD matrix, where the rows are the ground-truths,
# and the columns are the EVD with respect to the reward functions
function log_evd!(_log, mdp, Î¸s, ground_truth)
    # EVD is how well does a policy generated by r' behave on an MDP
    # with the true reward function r
    vs = ground_truth[:vs]
    EVD_matrix = zeros(size(vs,1), size(Î¸s,1))
    for (i,v) in enumerate(vs)
        # Make the MDP with the real reward values of the ith agent
        tmp_mdp = copy(mdp)
        tmp_mdp.reward_values = ground_truth[:rewards][i]
        for (j,Î¸) in enumerate(Î¸s)
            # Solve the MDP with our reward function and get the optimal Ï€
            Ï€áµ£ = solve_mdp(mdp, Î¸)
            # Check how well does Ï€ work w.r.t. the optimal value function
            váµ£ = policy_evaluation(tmp_mdp, Ï€áµ£)
            EVD_matrix[i,j] = norm(v - váµ£,1)
        end
    end
    push!(_log, EVD_matrix)
end


"""
    (proportional) Likelihood function for a single state action
    Normally should have normalisiation, but not important when calculating âˆ‡ğ“›
"""
state_action_lh(Ï€áµ¦, s,a) = Ï€áµ¦[s,a]

"""
    Executes a maximum likelihood inverse reinforcement learning
    mdp:            the problem
    Ï•:              operator to features space
    trajectories:   experts' trajectories
    iterations:     number of iterations to run for
    Î±:              learning rate
    Î²:              confidence parameter
    Îº:              concentration parameter for DPM
    burn_in:        number of iterations not to record (at the beginning)
"""
function DPM_BIRL(mdp, Ï•, Ï‡, iterations; Î±=0.1, Îº=1., Î²=0.5, ground_truth = nothing, verbose=true, update=:ML, burn_in=5)

    verbose ? println("Using $(update) update") : nothing

    Ï„ = sqrt(2*Î±)

    Î³ = mdp.discount_factor
    states = ordered_states(mdp)

    n_states  = size(states,1)-1
    n_actions = size( actions(mdp),1 )
    n_features = size(Ï•,2)
    n_trajectories = size(Ï‡,1)

    # Precpmputes transition matrix for all actions
    # (independent of features)
    Pâ‚ = a2transition.(mdp,actions(mdp))
    actions_i = action_index.(mdp, actions(mdp))

    const glb = Globals(n_states, n_actions, n_features, n_trajectories, actions_i, Î², Î³, Pâ‚, Ï‡, Ï•)

    #### Initialisation ####
    # Initialise clusters
    # K = n_trajectories
    K = 5
    # assignements    = collect(1:n_trajectories)
    assignements    = rand(1:K, n_trajectories)
    # assignements = fill(1,n_trajectories)

    N = map(x->sum(assignements .== x), 1:K)


    # Prepare reward functions
    Î¸s = [sample(RewardFunction, n_features) for i in 1:K]
    for (k,Î¸) in enumerate(Î¸s)
        assigned2cluster = (assignements .== k)
        Ï‡â‚– = Ï‡[assigned2cluster]
        update_reward!(Î¸, mdp, Ï‡â‚–, glb)
    end

    ğ“›_traj = ones(n_trajectories)*1e-5
    c      = Clusters(K, assignements, N, ğ“›_traj, Î¸s)

    update_clusters!(c, mdp, Îº, glb)

    _log = Dict(:assignements => [], :EVDs => [], :likelihoods => [], :rewards => [], :clusters=>[])

    for t in 1:iterations
        tic()

        updated_clusters_id = update_clusters!(c, mdp, Îº, glb)
        verbose ? println("Clusters changed: $(length(updated_clusters_id)) of $(c.K)") : nothing

        for (k, Î¸) in enumerate(c.rewards)
            # Get the clusters' trajectories
            assigned2cluster = (c.assignements .== k)
            Ï‡â‚– = Ï‡[assigned2cluster]

            # Update likelihood and gradient to current cluster
            # TODO: only do this step if cluster changed
            if k in updated_clusters_id
                Î¸.ğ“›, Î¸.âˆ‡ğ“› = calâˆ‡ğ“›(mdp, Î¸.invT, Î¸.Ï€áµ¦,  Ï‡â‚–, glb)
            end

            # Find potential new reward
            if update == :langevin_rand
                Ïµ = Î± * rand(Normal(0,1), n_features)
                indeces = rand(n_features) .< 0.2
                Ïµ[indeces] = 0.0
                Î¸â» = Î¸ + Î± * Î¸.âˆ‡ğ“› + Ïµ
            else
                Î¸â» = Î¸ + Î± * Î¸.âˆ‡ğ“›
            end

            # Solve everything for potential new reward
            Ï€â»  = solve_mdp(mdp, Î¸â»)
            Ï€áµ¦â» = calÏ€áµ¦(mdp, Ï€â».qmat, glb)
            invTâ» = calInvTransition(mdp, Ï€áµ¦â», Î³)

            # Calculate likelihood and gradient for new reward
            ğ“›â», âˆ‡ğ“›â» = calâˆ‡ğ“›(mdp, invTâ», Ï€áµ¦â»,  Ï‡â‚–, glb)


            # Do the update
            if update == :ML
                # We simply follow the gradient
                Î¸.values, Î¸.ğ“›, Î¸.âˆ‡ğ“›, Î¸.invT, Î¸.Ï€, Î¸.Ï€áµ¦ = Î¸â».values, ğ“›â», âˆ‡ğ“›â», invTâ», Ï€â», Ï€áµ¦â»
            elseif update == :langevin || update == :langevin_rand
                # Use result from Choi
                Î¸.ğ“› += sum(logpdf.(Normal(0,1), Î¸.values))
                ğ“›â» += sum(logpdf.(Normal(0,1), Î¸â».values))
                # logpdâ» = Base.log(proposal_distribution(Î¸â», Î¸, âˆ‡ğ“›â», Ï„))
                # logpd  = Base.log(proposal_distribution(Î¸, Î¸â», Î¸.âˆ‡ğ“›, Ï„))
                logpdâ» = proposal_distribution(Î¸â», Î¸, âˆ‡ğ“›â», Ï„)
                logpd = proposal_distribution(Î¸, Î¸â», Î¸.âˆ‡ğ“›, Ï„)

                # print("($(@sprintf("%.2f", Î¸.ğ“›)), $(@sprintf("%.2f", ğ“›â»)), $(@sprintf("%.2f", logpd)), $(@sprintf("%.2f", logpdâ»)))")

                p =  ğ“›â» / Î¸.ğ“› * logpdâ» / logpd
                # p = exp( ğ“›â» + logpdâ» - Î¸.ğ“› - logpd)
                print("$p, ")
                if rand() < p
                    Î¸.values, Î¸.ğ“›, Î¸.âˆ‡ğ“›, Î¸.invT, Î¸.Ï€, Î¸.Ï€áµ¦ = Î¸â».values, ğ“›â», âˆ‡ğ“›â», invTâ», Ï€â», Ï€áµ¦â»
                end
            end
        end

        elapsed = toq()

        # Log EVD
        verbose ? println("Iteration number $t took $elapsed seconds") : nothing
        if t > burn_in
            push!(_log[:assignements], copy(c.N))
            push!(_log[:likelihoods], map(x->x.ğ“›, c.rewards))
            push!(_log[:rewards], copy(c.rewards))
            push!(_log[:clusters], copy(c))

            if ground_truth !== nothing
                log_evd!(_log[:EVDs], mdp, c.rewards, ground_truth)
                verbose ? show(_log[:EVDs][end]) : nothing
            end
        end
    end

    c, _log
end



# End module
end
