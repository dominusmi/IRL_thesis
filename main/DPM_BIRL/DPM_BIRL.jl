module DPMBIRL

export DPMBIRL, generate_gridworld, generate_trajectories

using POMDPs
using POMDPModels
using Distributions
using POMDPToolbox
using JLD

immutable Globals
    n_states::Int64
    n_actions::Int64
    n_features::Int64
    n_trajectories::Int64
    actions_i::Array{Int64}
    Î²::Float64
    Î³::Float64
    Pâ‚::Array{Array{Float64,2},1}
    Ï‡::Array{MDPHistory}
    Ï•::Array{Float64,2}
end

include("../reward.jl")
include("../cluster.jl")
include("../utilities/gridworld.jl")
include("../utilities/policy.jl")
include("../utilities/general.jl")
include("../utilities/trajectory.jl")

# Logs a EVD matrix, where the rows are the ground-truths,
# and the columns are the EVD with respect to the reward functions
function log_evd!(_log, mdp, Î¸s, ground_truth)
    # EVD is how well does a policy generated by r' behave on an MDP
    # with the true reward function r
    vs = ground_truth[:vs]
    EVD_matrix = zeros(size(vs,1), size(Î¸s,1))
    for (i,v) in enumerate(vs)
        # Make the MDP with the real reward values of the ith agent
        tmp_mdp = copy(mdp)
        tmp_mdp.reward_values = ground_truth[:rewards][i]
        for (j,Î¸) in enumerate(Î¸s)
            # Solve the MDP with our reward function and get the optimal Ï€
            Ï€áµ£ = solve_mdp(mdp, Î¸)
            # Check how well does Ï€ work w.r.t. the optimal value function
            váµ£ = policy_evaluation(tmp_mdp, Ï€áµ£)
            EVD_matrix[i,j] = norm(v - váµ£,2)
        end
    end
    push!(_log, EVD_matrix)
end


"""
    (proportional) Likelihood function for a single state action
    Normally should have normalisiation, but not important when calculating âˆ‡ğ“›
"""
state_action_lh(Ï€áµ¦, s,a) = Ï€áµ¦[s,a]

"""
    Executes a maximum likelihood inverse reinforcement learning
    mdp:            the problem
    Ï•:              operator to features space
    trajectories:   experts' trajectories
    iterations:     number of iterations to run for
    Î±:              learning rate
    Î²:              confidence parameter
    Îº:              concentration parameter for DPM
    burn_in:        number of iterations not to record (at the beginning)
"""
function DPM_BIRL(mdp, Ï•, Ï‡, iterations; Î±=0.1, Îº=1., Î²=0.5, ground_truth = nothing, verbose=true, update=:ML, burn_in=5, use_clusters=true, path_to_file=nothing)

    verbose ? println("Using $(update) update") : nothing

    Ï„ = sqrt(2*Î±)

    Î³ = mdp.discount_factor
    states = ordered_states(mdp)

    n_states  = size(states,1)-1
    n_actions = size( actions(mdp),1 )
    n_features = size(Ï•,2)
    n_trajectories = size(Ï‡,1)

    # Precpmputes transition matrix for all actions
    # (independent of features)
    Pâ‚ = a2transition.(mdp,actions(mdp))
    actions_i = action_index.(mdp, actions(mdp))

    const glb = Globals(n_states, n_actions, n_features, n_trajectories, actions_i, Î², Î³, Pâ‚, Ï‡, Ï•)

    #### Initialisation ####
    # Initialise clusters
    # K = n_trajectories
    # K = 5
    K = 1
    # assignements    = collect(1:n_trajectories)
    assignements    = rand(1:K, n_trajectories)
    # assignements = fill(1,n_trajectories)

    N = map(x->sum(assignements .== x), 1:K)


    # Prepare reward functions
    Î¸s = [sample(RewardFunction, n_features) for i in 1:K]
    for (k,Î¸) in enumerate(Î¸s)
        assigned2cluster = (assignements .== k)
        Ï‡â‚– = Ï‡[assigned2cluster]
        update_reward!(Î¸, mdp, Ï‡â‚–, glb)
    end

    ğ“›_traj = ones(n_trajectories)*1e-5
    c      = Clusters(K, assignements, N, ğ“›_traj, Î¸s)

    use_clusters ? update_clusters!(c, mdp, Îº, glb) : nothing

    _log = Dict(:assignements => [], :EVDs => [], :likelihoods => [], :rewards => [], :clusters=>[], :acceptance_probability=>[])

    Ïƒ = eye(n_features)*0.1
    burned = 0
    for t in 1:iterations
        changed = false
        tic()

        if use_clusters
            updated_clusters_id = Set()
            updated_clusters_id = update_clusters!(c, mdp, Îº, glb)
            verbose ? println("Clusters changed: $(length(updated_clusters_id)) of $(c.K)") : nothing
        end

        for (k, Î¸) in enumerate(c.rewards)
            # Get the clusters' trajectories
            assigned2cluster = (c.assignements .== k)
            Ï‡â‚– = Ï‡[assigned2cluster]

            # Update likelihood and gradient to current cluster
            # Î¸.ğ“› = calğ“›(mdp, Î¸.Ï€áµ¦, Ï‡â‚–)
            # Î¸.âˆ‡ğ“› = calâˆ‡ğ“›(mdp, Î¸.invT, Î¸.Ï€áµ¦,  Ï‡â‚–, glb)
            if use_clusters && k âˆˆ updated_clusters_id
                Î¸.ğ“› = calğ“›(mdp, Î¸.Ï€áµ¦, Ï‡â‚–, glb)
                Î¸.âˆ‡ğ“› = calâˆ‡ğ“›(mdp, Î¸.invT, Î¸.Ï€áµ¦,  Ï‡â‚–, glb)
            elseif update == :MH
                Î¸.ğ“› = calğ“›(mdp, Î¸.Ï€áµ¦, Ï‡â‚–, glb)
            end

            # Find potential new reward
            if update == :langevin_rand
                Ïµ = rand(Normal(0,1), n_features)
                # indeces = rand(n_features) .< 0.2
                # Ïµ[indeces] = 0.0
                Î¸â» = Î¸ + Î±*Î¸.âˆ‡ğ“› + Ïµ*Î±
                Î¸â».values ./= sum(abs.(Î¸â».values))
            elseif update == :MH
                # Ïµ = rand(Normal(0,1), n_features)
                Ïµ = rand(MultivariateNormal(Ïƒ))
                Î¸â» = Î¸ + Ïµ
            else
                Î¸â» = Î¸ + Î±*Î¸.âˆ‡ğ“›
                Î¸â».values ./= sum(abs.(Î¸â».values))
            end

            # Solve everything for potential new reward
            Ï€â»  = solve_mdp(mdp, Î¸â»)
            Ï€áµ¦â» = calÏ€áµ¦(mdp, Ï€â».qmat, glb)
            ğ“›â» = calğ“›(mdp, Ï€áµ¦â», Ï‡â‚–, glb)

            if update !== :MH
                invTâ» = calInvTransition(mdp, Ï€áµ¦â», Î³)
                âˆ‡ğ“›â» = calâˆ‡ğ“›(mdp, invTâ», Ï€áµ¦â»,  Ï‡â‚–, glb)
            end


            # Do the update
            if update == :ML
                # We simply follow the gradient
                # logPriorâ», âˆ‡logPriorâ» = log_prior(Î¸â»)
                # ğ“›â» += logPriorâ»
                # âˆ‡ğ“›â» += âˆ‡logPriorâ»
                println("log ğ“›: $(@sprintf("%.2f", Î¸.ğ“›)), log ğ“›â»: $(@sprintf("%.2f", ğ“›â»))")
                Î¸.values, Î¸.ğ“›, Î¸.âˆ‡ğ“›, Î¸.invT, Î¸.Ï€, Î¸.Ï€áµ¦ = Î¸â».values, ğ“›â», âˆ‡ğ“›â», invTâ», Ï€â», Ï€áµ¦â»
            elseif update == :MH
                logPrior, ~ = log_prior(Î¸)
                logPriorâ», ~ = log_prior(Î¸â»)
                Î¸.ğ“› += logPrior
                ğ“›â» += logPriorâ»

                # println("log ğ“›: $(@sprintf("%.2f", Î¸.ğ“›)), log ğ“›â»: $(@sprintf("%.2f", ğ“›â»))")

                p = exp(ğ“›â» - Î¸.ğ“›)
                # println("   current p: $p")
                if rand() < p
                    Î¸.values, Î¸.ğ“›, Î¸.âˆ‡ğ“›, Î¸.invT, Î¸.Ï€, Î¸.Ï€áµ¦ = Î¸â».values, ğ“›â», zeros(0), zeros(0,0), Ï€â», Ï€áµ¦â»
                    changed = true
                    burned += 1
                end
            elseif update == :langevin || update == :langevin_rand
                # Use result from Choi

                logPrior, âˆ‡logPrior = log_prior(Î¸)
                logPriorâ», âˆ‡logPriorâ» = log_prior(Î¸â»)

                println("    before prior log ğ“›: ($(@sprintf("%.2f", Î¸.ğ“›)), log ğ“›â»: $(@sprintf("%.2f", ğ“›â»))")

                Î¸.ğ“› += logPrior
                Î¸.âˆ‡ğ“› += âˆ‡logPrior
                ğ“›â» += logPriorâ»
                âˆ‡ğ“›â» += âˆ‡logPriorâ»

                #### CHOI SHIT ####

                # a = Ïµ + Ï„/2*(Î¸.âˆ‡ğ“› + âˆ‡ğ“›â»)
                # a = exp(-0.5*sum(a.^2))*exp(ğ“›â»);
                # b = exp(-0.5 * sum(Ïµ.^2) ) * exp( Î¸.ğ“› )

                # @show Î¸.ğ“›, ğ“›â», a, b, norm(Ïµ + Ï„/2*(Î¸.âˆ‡ğ“› - âˆ‡ğ“›â»))^2

                # p = a/b

                #### CURRENT WORKING VERSION ####
                logpdâ» = proposal_distribution(Î¸â», Î¸, âˆ‡ğ“›â», Ï„)
                logpd = proposal_distribution(Î¸, Î¸â», Î¸.âˆ‡ğ“›, Ï„)

                log_coef = log(inv(2*3.1415*Ï„^2)^(n_features/2))

                println("log ğ“›: ($(@sprintf("%.2f", Î¸.ğ“›)), log ğ“›â»: $(@sprintf("%.2f", ğ“›â»)), logpd: $(@sprintf("%.2f", logpd)), logpdâ»: $(@sprintf("%.2f", logpdâ»)))")
                # print("ğ“›: ($(@sprintf("%.2f", exp(Î¸.ğ“›))), ğ“›â» $(@sprintf("%.2f", exp(ğ“›â»))), $(@sprintf("%.2f", log_coef+logpd)), $(@sprintf("%.2f", log_coef+logpdâ»)))")


                p = exp(ğ“›â»-Î¸.ğ“› + logpdâ»-logpd)
                # p =  (ğ“›â»/Î¸.ğ“›) * logpdâ» / logpd
                # p =  ğ“›â» / Î¸.ğ“› * logpdâ» / logpd


                # p = percentage_likelihood * logpdâ» / logpd
                # p = exp( ğ“›â» + logpdâ» - Î¸.ğ“› - logpd)
                println("   current p: $p")
                # println("   real p:    $( exp(ğ“›â» - Î¸.ğ“›) * exp(log_coef+logpdâ» - log_coef-logpd))")
                if rand() < p
                    Î¸.values, Î¸.ğ“›, Î¸.âˆ‡ğ“›, Î¸.invT, Î¸.Ï€, Î¸.Ï€áµ¦ = Î¸â».values, ğ“›â», âˆ‡ğ“›â», invTâ», Ï€â», Ï€áµ¦â»
                end
            end
        end

        elapsed = toq()

        if changed
            println("Burned: $burned")
        end

        # Log EVD
        verbose ? println("Iteration number $t took $elapsed seconds") : nothing
        if burned > burn_in
            # push!(_log[:assignements], copy(c.N))
            if path_to_file == nothing
                push!(_log[:likelihoods], map(x->x.ğ“›, c.rewards))
                push!(_log[:rewards], copy(c.rewards))
                use_clusters ? push!(_log[:clusters], copy(c)) : nothing

                if ground_truth !== nothing
                    log_evd!(_log[:EVDs], mdp, c.rewards, ground_truth)
                    verbose ? show(_log[:EVDs][end]) : nothing
                end
            elseif path_to_file !== nothing && changed
                f = jldopen(path_to_file, "r+")
                write(f, "reward_$burned", c.rewards[1].values)
                write(f, "likelihood_$burned", c.rewards[1].ğ“›)
                close(f)
            end
        elseif burned < burn_in
            push!(_log[:rewards], c.rewards)
        elseif burned == burn_in
            push!(_log[:rewards], c.rewards)
            println("Finished burn in")
            rewards = zeros(burn_in, n_features)
            @show size(_log[:rewards])
            for i in 1:burn_in
                rewards[i,:] = _log[:rewards][i][1].values
            end
            # Ïƒ = Ïƒ .* [sqrt(cov(rewards[rewards[:,1].!==0.0,i])) for i in 1:n_features]
            _log[:rewards] = []
            # @show Ïƒ
            # println("Found new covariance, sample: $(Ïƒ[1:3,1:3])")
            burn_in = 0
            burned = 1
            f = jldopen(path_to_file, "w")
            write(f, "reward_$burned", c.rewards[1])
            close(f)
        end
    end

    c, _log
end



# End module
end
