using POMDPs
using POMDPModels
using Distributions
using POMDPToolbox


include("../reward.jl")
include("../cluster.jl")
include("../utilities/gridworld.jl")
include("../utilities/policy.jl")
include("../utilities/general.jl")
include("../utilities/trajectory.jl")




"""
    (proportional) Likelihood function for a single state action
    Normally should have normalisiation, but not important when calculating âˆ‡ğ“›
"""
state_action_lh(Ï€áµ¦, s,a) = Ï€áµ¦[s,a]

"""
    Executes a maximum likelihood inverse reinforcement learning
    mdp:            the problem
    Ï•:              operator to features space
    trajectories:   experts' trajectories
    iterations:     number of iterations to run for
    Î±:              learning rate
    Î²:              confidence parameter
    Îº:              concentration parameter for DPM
"""
function DPM_BIRL(mdp, Ï•, Ï‡, iterations; Î±=0.1, Îº=1., Î²=0.5, ground_policy = nothing, verbose=true, update=:ML)

    verbose ? println("Using $(update) update") : nothing

    Ï„ = sqrt(2*Î±)

    Î³ = mdp.discount_factor
    states = ordered_states(mdp)

    n_states  = size(states,1)-1
    n_actions = size( actions(mdp),1 )
    n_features = size(Ï•,2)
    n_trajectories = size(Ï‡,1)

    # Initialise random reward function
    Î¸ = sample(RewardFunction, n_features)

    EVD = []

    if verbose && ground_policy !== nothing
        v = policy_evaluation(mdp, ground_policy)
    end

    # Precpmputes transition matrix for all actions
    # (independent of features)
    Pâ‚ = a2transition.(mdp,actions(mdp))
    actions_i = action_index.(mdp, actions(mdp))

    #### Initialisation ####
    # Initialise clusters
    K = 1

    # Prepare reward functions
    # Î¸s              = [sample(RewardFunction, mdp.size_x*mdp.size_y) for i in 1:K]
    # for Î¸ in Î¸s
    #     # Solve mdp with current reward
    #     Î¸.Ï€  = solve_mdp(mdp, Î¸)
    #     # Find Boltzmann policy
    #     Î¸.Ï€áµ¦ = calÏ€áµ¦(mdp, Ï€.qmat, Î²)
    #
    #     # Prepare variables for gradient
    #     Î¸.invT = calInvTransition(mdp, Ï€áµ¦, Î³)
    #     # Calculates value and gradient of trajectory likelihood
    #     Î¸.ğ“›, Î¸.âˆ‡ğ“› = calâˆ‡ğ“›(mdp, Ï•, Î¸.invT, Pâ‚, Î¸.Ï€áµ¦, Î², Ï‡, n_states, n_actions, n_features, actions_i)
    # end
    #
    # assignements    = rand(1:K, n_trajectories)
    # N               = map(x->sum(assignements .== x), 1:K)
    # ğ“›               = ones(K)*1e-5
    # c               = Clusters(K, assignements, N, ğ“›, Î¸s)
    #
    # update_clusters!(c, Ï‡, Îº, Î²)

    # Solve mdp with current reward
    Î¸.Ï€  = solve_mdp(mdp, Î¸)
    # Find Boltzmann policy
    Î¸.Ï€áµ¦ = calÏ€áµ¦(mdp, Î¸.Ï€.qmat, Î²)

    # Prepare variables for gradient
    Î¸.invT = calInvTransition(mdp, Î¸.Ï€áµ¦, Î³)
    # Calculates value and gradient of trajectory likelihood
    Î¸.ğ“›, Î¸.âˆ‡ğ“› = calâˆ‡ğ“›(mdp, Ï•, Î¸.invT, Pâ‚, Î¸.Ï€áµ¦, Î², Ï‡, n_states, n_actions, n_features, actions_i)

    for t in 1:30
        tic()

        # Find potential new reward
        if update == :langevin_rand
            Î¸â» = Î¸ + Î± * âˆ‡ğ“› + Î± * rand(Normal(0,1), n_features)
        else
            Î¸â» = Î¸ + Î± * Î¸.âˆ‡ğ“›
        end

        # Solve everything for potential new reward
        Ï€â»  = solve_mdp(mdp, Î¸â»)
        Ï€áµ¦â» = calÏ€áµ¦(mdp, Ï€â».qmat, Î²)

        invTâ» = calInvTransition(mdp, Ï€áµ¦â», Î³)
        ğ“›â», âˆ‡ğ“›â» = calâˆ‡ğ“›(mdp, Ï•, invTâ», Pâ‚, Ï€áµ¦â», Î², Ï‡, n_states, n_actions, n_features, actions_i)

        # Do the update
        if update == :ML
            # We simply follow the gradient
            Î¸.values, Î¸.ğ“›, Î¸.âˆ‡ğ“›, Î¸.invT, Î¸.Ï€, Î¸.Ï€áµ¦ = Î¸â».values, ğ“›â», âˆ‡ğ“›â», invTâ», Ï€â», Ï€áµ¦â»
        elseif update == :langevin || update == :langevin_rand
            # Use result from Choi
            ğ“› += sum(pdf.(Normal(0,1), Î¸.values))
            ğ“›â» += sum(pdf.(Normal(0,1), Î¸â».values))
            p =  ğ“›â» / ğ“› * proposal_distribution(Î¸â», Î¸, âˆ‡ğ“›â», Ï„) / proposal_distribution(Î¸, Î¸â», âˆ‡ğ“›, Ï„)
            @show p
            if rand() > p
                Î¸.values, Î¸.ğ“›, Î¸.âˆ‡ğ“›, Î¸.invT, Î¸.Ï€, Î¸.Ï€áµ¦ = Î¸â».values, ğ“›â», âˆ‡ğ“›â», invTâ», Ï€â», Ï€áµ¦â»
            end
        end

        elapsed = toq()

        # Log EVD
        if verbose
            println("Iteration took $elapsed seconds")
            if ground_policy !== nothing
                # Need to change this to account for features
                váµ£ = policy_evaluation(mdp, Î¸.Ï€)
                push!(EVD, norm(v-váµ£))
            end
        end
    end

    # Log EVD
    if verbose && ground_policy !== nothing
        # Need to change this to account for features
        Ï€áµ£ = solve_mdp(mdp, Î¸)
        váµ£ = policy_evaluation(mdp, Ï€áµ£)
        push!(EVD, norm(v-váµ£))
        println("Final EVD: $(EVD[end])")
    end

    Î¸, EVD
end




function calâˆ‡ğ“›(mdp, Ï•, invT, Pâ‚, Ï€áµ¦, Î², Ï‡, n_states, n_actions, n_features, actions_i)
    ğ“›  = 0.
    âˆ‡ğ“› = zeros(n_features)
    for k in 1:n_features
        dQâ‚– = zeros( n_states, n_actions )
        caldQâ‚–!(dQâ‚–, mdp, Ï•, invT, Pâ‚, Ï€áµ¦, k)

        # Calculates total gradient over trajectories
        for (m,trajectory) in enumerate(Ï‡)
            for (h,state) in enumerate(trajectory.state_hist[1:end-1])
                sâ‚• = state_index(mdp, state)
                aâ‚• = action_index(mdp, trajectory.action_hist[h])

                ğ“› += state_action_lh(Ï€áµ¦,sâ‚•,aâ‚•)

                dl_dÎ¸â‚– = Î² * ( dQâ‚–[sâ‚•,aâ‚•] - sum( [ state_action_lh(Ï€áµ¦,sâ‚•,aiâ») * dQâ‚–[sâ‚•,aiâ»] for aiâ» in actions_i ] ) )
                âˆ‡ğ“›[k] += dl_dÎ¸â‚–
            end
        end
    end
    ğ“›, âˆ‡ğ“›
end
