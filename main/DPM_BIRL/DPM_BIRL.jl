module DPMBIRL

export DPMBIRL, generate_gridworld, generate_trajectories, RewardFunction

using POMDPs
using POMDPModels
using Distributions
using POMDPToolbox
using JLD

immutable Globals
    n_states::Int64
    n_actions::Int64
    n_features::Int64
    n_trajectories::Int64
    actions_i::Array{Int64}
    Î²::Float64
    Î³::Float64
    Pâ‚::Array{Array{Float64,2},1}
    Ï‡::Array{MDPHistory}
    Ï•::Array{Float64,2}
end

immutable LoggedFloat <: AbstractFloat
    value::AbstractFloat
    last_modified::Integer
    LoggedFloat(x) = new(x,0)
    LoggedFloat(x::AbstractFloat, y::Integer) = new(x,y)
end

include("../reward.jl")
include("../cluster.jl")
include("../utilities/gridworld.jl")
include("../utilities/policy.jl")
include("../utilities/general.jl")
include("../utilities/trajectory.jl")
include("../utilities/logging.jl")


# Logs a EVD matrix, where the rows are the ground-truths,
# and the columns are the EVD with respect to the reward functions
function log_evd!(_log, mdp, Î¸s, ground_truth)
    # EVD is how well does a policy generated by r' behave on an MDP
    # with the true reward function r
    vs = ground_truth[:vs]
    EVD_matrix = zeros(size(vs,1), size(Î¸s,1))
    for (i,v) in enumerate(vs)
        # Make the MDP with the real reward values of the ith agent
        tmp_mdp = copy(mdp)
        tmp_mdp.reward_values = ground_truth[:rewards][i]
        for (j,Î¸) in enumerate(Î¸s)
            # Solve the MDP with our reward function and get the optimal Ï€
            Ï€áµ£ = solve_mdp(mdp, Î¸)
            # Check how well does Ï€ work w.r.t. the optimal value function
            váµ£ = policy_evaluation(tmp_mdp, Ï€áµ£)
            EVD_matrix[i,j] = norm(v - váµ£,2)
        end
    end
    push!(_log, EVD_matrix)
end


"""
    (proportional) Likelihood function for a single state action
    Normally should have normalisiation, but not important when calculating âˆ‡ğ“›
"""
state_action_lh(Ï€áµ¦, s,a) = Ï€áµ¦[s,a]

"""
    Executes a maximum likelihood inverse reinforcement learning
    mdp:            the problem
    Ï•:              operator to features space
    trajectories:   experts' trajectories
    iterations:     number of iterations to run for
    Î±:              learning rate
    Î²:              confidence parameter
    Îº:              concentration parameter for DPM
    burn_in:        number of iterations not to record (at the beginning)
"""
function DPM_BIRL(mdp, Ï•, Ï‡, iterations; Ï„=0.1, Îº=1., Î²=0.5, ground_truth = nothing, verbose=true, update=:ML, burn_in=5,
                    use_clusters=true, path_to_folder=nothing, seed=1, use_prior=true, parameters=nothing)

    srand(seed)
    verbose ? println("Using $(update) update") : nothing

    if parameters !== nothing && path_to_folder !== nothing
        path_to_file = prepare_JLD_log_file(path_to_folder, parameters)
    end

    Î± = Ï„^2/2

    Î³ = mdp.discount_factor
    states = ordered_states(mdp)

    n_states  = size(states,1)-1
    n_actions = size( actions(mdp),1 )
    n_features = size(Ï•,2)
    n_trajectories = size(Ï‡,1)

    # Precpmputes transition matrix for all actions
    Pâ‚ = a2transition.(mdp,actions(mdp))
    actions_i = action_index.(mdp, actions(mdp))

    const glb = Globals(n_states, n_actions, n_features, n_trajectories, actions_i, Î², Î³, Pâ‚, Ï‡, Ï•)

    #### Initialisation ####
    # Initialise clusters
    if use_clusters
        K = n_trajectories
        assignements    = collect(1:n_trajectories)
    else
        K = 1
        assignements = fill(1,n_trajectories)
    end
    # K = 5
    # K = 1
    # assignements    = rand(1:K, n_trajectories)
    # assignements = fill(1,n_trajectories)

    N = map(x->sum(assignements .== x), 1:K)


    # Prepare reward functions
    Î¸s = [sample(RewardFunction, n_features) for i in 1:K]
    for (k,Î¸) in enumerate(Î¸s)
        assigned2cluster = (assignements .== k)
        Ï‡â‚– = Ï‡[assigned2cluster]
        update_reward!(Î¸, mdp, Ï‡â‚–, glb)
    end



    # Prepares clusters
    ğ“›_traj = ones(n_trajectories)*1e-5
    c      = Clusters(K, assignements, N, ğ“›_traj, Î¸s)

    use_clusters ? update_clusters!(c, mdp, Îº, glb) : nothing

    # Prepares log variable
    _log = Dict(:assignements => [], :EVDs => [], :likelihoods => [], :rewards => [], :clusters=>[], :acceptance_probability=>[], :acc_prob=>[])
    # changed_log = Array{Bool}(iterations) .= false
    changed_log = zeros(iterations)
    # General variables
    Ïƒ = eye(n_features)*Ï„
    burned = 0
    probabilities = []

    #### MAIN LOOP ####
    for t in 1:iterations
        tic()
        changed_counter = 0

        tic()
        if use_clusters
            updated_clusters_id = Set()
            updated_clusters_id = update_clusters!(c, mdp, Îº, glb)
            verbose ? println("Clusters changed: $(length(updated_clusters_id)) of $(c.K)") : nothing
        end
        cluster_time = toq()
        println("Cluster update took $cluster_time seconds")


        for (k, Î¸) in enumerate(c.rewards)
            # Get the clusters' trajectories
            assigned2cluster = (c.assignements .== k)
            Ï‡â‚– = Ï‡[assigned2cluster]

            # Update likelihood and gradient to current cluster
            # Î¸.ğ“› = calğ“›(mdp, Î¸.Ï€áµ¦, Ï‡â‚–)
            # Î¸.âˆ‡ğ“› = calâˆ‡ğ“›(mdp, Î¸.invT, Î¸.Ï€áµ¦,  Ï‡â‚–, glb)
            if use_clusters && k âˆˆ updated_clusters_id
                Î¸.ğ“› = calğ“›(mdp, Î¸.Ï€áµ¦, Ï‡â‚–, glb)
                Î¸.âˆ‡ğ“› = calâˆ‡ğ“›(mdp, Î¸.invT, Î¸.Ï€áµ¦,  Ï‡â‚–, glb)
            elseif update == :MH
                Î¸.ğ“› = calğ“›(mdp, Î¸.Ï€áµ¦, Ï‡â‚–, glb)
            else
                Î¸.ğ“› = calğ“›(mdp, Î¸.Ï€áµ¦, Ï‡â‚–, glb)
                Î¸.âˆ‡ğ“› = calâˆ‡ğ“›(mdp, Î¸.invT, Î¸.Ï€áµ¦,  Ï‡â‚–, glb)
            end

            # Find potential new reward
            if update == :langevin_rand
                Ïµ = rand(Normal(0,1), n_features)
                indeces = rand(n_features) .< 0.2
                Ïµ[indeces] = 0.0
                Î¸â» = Î¸ + Î±*Î¸.âˆ‡ğ“› + Ï„*Ïµ
                Î¸â».weights ./= sum(abs.(Î¸â».weights))
            elseif update == :MH
                # Ïµ = rand(Normal(0,1), n_features)
                Ïµ = rand(MultivariateNormal(Ïƒ))
                Î¸â» = Î¸ + Ïµ
            else
                Î¸â» = Î¸ + Î±*Î¸.âˆ‡ğ“›
                Î¸â».weights ./= sum(abs.(Î¸â».weights))
            end
            Î¸â».values = values(Î¸â», glb.Ï•)

            # Solve everything for potential new reward
            Ï€â»  = solve_mdp(mdp, Î¸â»)
            Ï€áµ¦â» = calÏ€áµ¦(mdp, Ï€â».qmat, glb)
            ğ“›â» = calğ“›(mdp, Ï€áµ¦â», Ï‡â‚–, glb)

            if update !== :MH
                invTâ» = calInvTransition(mdp, Ï€áµ¦â», Î³)
                âˆ‡ğ“›â» = calâˆ‡ğ“›(mdp, invTâ», Ï€áµ¦â»,  Ï‡â‚–, glb)
            end


            # Do the update
            p = 0.
            if update == :ML
                # We simply follow the gradient
                # logPriorâ», âˆ‡logPriorâ» = log_prior(Î¸â»)
                # ğ“›â» += logPriorâ»
                # âˆ‡ğ“›â» += âˆ‡logPriorâ»
                p=1.0
                # println("log ğ“›: $(@sprintf("%.2f", Î¸.ğ“›)), log ğ“›â»: $(@sprintf("%.2f", ğ“›â»))")
            elseif update == :MH
                logPrior, ~ = log_prior(Î¸)
                logPriorâ», ~ = log_prior(Î¸â»)
                Î¸.ğ“› += logPrior
                ğ“›â» += logPriorâ»
                âˆ‡ğ“›â» = zeros(0)
                invTâ» = zeros(0,0)
                # println("log ğ“›: $(@sprintf("%.2f", Î¸.ğ“›)), log ğ“›â»: $(@sprintf("%.2f", ğ“›â»))")
                p = exp(ğ“›â» - Î¸.ğ“›)
            elseif update == :langevin || update == :langevin_rand
                # Use result from Choi
                logPrior, âˆ‡logPrior = log_prior(Î¸)
                logPriorâ», âˆ‡logPriorâ» = log_prior(Î¸â»)

                Î¸.ğ“› += logPrior
                Î¸.âˆ‡ğ“› += âˆ‡logPrior
                ğ“›â» += logPriorâ»
                âˆ‡ğ“›â» += âˆ‡logPriorâ»


                #### CHOI SHIT ####

                # a = Ïµ + Ï„/2*(Î¸.âˆ‡ğ“› + âˆ‡ğ“›â»)
                # a = exp(-0.5*sum(a.^2))*exp(ğ“›â»);
                # b = exp(-0.5 * sum(Ïµ.^2) ) * exp( Î¸.ğ“› )

                # @show Î¸.ğ“›, ğ“›â», a, b, norm(Ïµ + Ï„/2*(Î¸.âˆ‡ğ“› - âˆ‡ğ“›â»))^2

                # p = a/b

                #### CURRENT WORKING VERSION ####
                logpdâ» = proposal_distribution(Î¸â», Î¸, âˆ‡ğ“›â», Ï„)
                logpd = proposal_distribution(Î¸, Î¸â», Î¸.âˆ‡ğ“›, Ï„)

                p = exp(ğ“›â»-Î¸.ğ“› + logpdâ»-logpd)
            end
            if p > 1. || rand() < p
                Î¸.weights, Î¸.ğ“›, Î¸.âˆ‡ğ“›, Î¸.invT, Î¸.Ï€, Î¸.Ï€áµ¦, Î¸.values = Î¸â».weights, ğ“›â», âˆ‡ğ“›â», invTâ», Ï€â», Ï€áµ¦â», values(Î¸â», glb.Ï•)
                burned += 1
                changed_counter += 1
                # avg_changed += 1
            end
            push!(_log[:acc_prob], p)
        end



        elapsed = toq()

        verbose ? println("Iteration number $t took $elapsed seconds") : nothing
        if t > burn_in
            # push!(_log[:assignements], copy(c.N))
            if path_to_folder == nothing
                # push!(_log[:likelihoods], map(x->x.ğ“›, c.rewards))
                push!(_log[:rewards], copy.(c.rewards))
                use_clusters ? push!(_log[:clusters], copy(c)) : nothing

                # if ground_truth !== nothing
                    # log_evd!(_log[:EVDs], mdp, c.rewards, ground_truth)
                    # verbose ? show(_log[:EVDs][end]) : nothing
                # end
            else
                f = jldopen(path_to_file, "r+")
                write(f, "rewards_$(t-burn_in)", r2weights.(c.rewards))
                close(f)
            end
        elseif t < burn_in
            # push!(_log[:likelihoods], map(x->x.ğ“›, c.rewards))
            # push!(_log[:rewards], copy.(c.rewards))
        elseif t == burn_in
            # push!(_log[:likelihoods], map(x->x.ğ“›, c.rewards))
            # push!(_log[:rewards], copy.(c.rewards))
            println("Finished burn in")
            ### Burn in covariance calculation ###
            # rewards = zeros(burn_in, n_features)
            # for i in 1:burn_in
                # rewards[i,:] = _log[:rewards][i][1].values
            # end
            # Ïƒ = Ïƒ .* [sqrt(cov(rewards[rewards[:,1].!==0.0,i])) for i in 1:n_features]
            # _log[:rewards] = []
            # @show Ïƒ
            # println("Found new covariance, sample: $(Ïƒ[1:3,1:3])")
        end

        # Update Ï„ to get acceptance rate between 0.4 and 0.8
        # Note: out of simplicity, changed refers to cluster #1 only
        # changed_log[t] = changed
        changed_log[t] = changed_counter / size(c.rewards,1)
        # @show changed_log[t]
        if t < burn_in
            Ï„ = update_Ï„(Ï„, t, changed_log)
        end

        # println("Current Ï„: $Ï„")
    end

    _log[:changed] = changed_log


    if path_to_file !== nothing
        f = jldopen(path_to_file, "r+")
        write(f, "acceptance_probabilities", _log[:acc_prob])
        close(f)
    end

    c, _log
end



# End module
end
