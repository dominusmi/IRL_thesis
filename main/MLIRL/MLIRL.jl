include("../reward.jl")
include("../utilities/gridworld.jl")
include("../utilities/policy.jl")
include("../utilities/general.jl")


"""
    (proportional) Likelihood function for a single state action
    Normally should have normalisiation, but not important when calculating âˆ‡ğ“›
"""
state_action_lh(Ï€áµ¦, s,a) = Ï€áµ¦[s,a]

"""
    Executes a maximum likelihood inverse reinforcement learning
    mdp:            the problem
    Ï•:              operator to features space
    trajectories:   experts' trajectories
    iterations:     number of iterations to run for
    Î±:              learning rate
    Î²:              confidence parameter
"""
function MLIRL(mdp, Ï•, trajectories, iterations; Î±=0.1, Î²=0.5, ground_policy = nothing, verbose=true)

    Î¸ = sample(RewardFunction, size(Ï•,2))
    Î³ = mdp.discount_factor
    states = ordered_states(mdp)

    n_states  = size(states,1)-1
    n_actions = size( actions(mdp),1 )
    n_features = size(Ï•,2)

    EVD = []

    if verbose && ground_policy !== nothing
        v = policy_evaluation(mdp, ground_policy)
    end

    for t in 1:iterations
        tic()
        # Ï€, Qáµ£   = calQáµ£(mdp, Î¸)
        Ï€  = solve_mdp(mdp, Î¸)
        Ï€áµ¦ = calÏ€áµ¦(mdp, Ï€.qmat, Î²)

        # Log EVD
        if verbose && ground_policy !== nothing
            tic()
            # Need to change this to account for features
            Ï€áµ£ = solve_mdp(mdp, Î¸)
            váµ£ = policy_evaluation(mdp, Ï€áµ£)
            push!(EVD, norm(v-váµ£))
            elapsed = toq()
        end


        invT = calInvTransition(mdp, Ï€áµ¦, Î³)
        actions_i = action_index.(mdp, actions(mdp))
        dQ = zeros(n_states, n_actions)
        âˆ‡ğ“› = SharedArray{Float64,1}(zeros(n_features))

        # Precpmputes transition matrix for all actions
        # (independent of features)
        Pâ‚ = a2transition.(mdp,actions(mdp))


        # Calculates gradient per feature
        for k in 1:n_features
            dQâ‚– = zeros( n_states, n_actions )
            caldQâ‚–!(dQâ‚–, mdp, Ï•, invT, Pâ‚, Ï€áµ¦, k)
            # Calculates total gradient over trajectories
            for (m,trajectory) in enumerate(trajectories)
                for (h,state) in enumerate(trajectory.state_hist[1:end-1])
                    sâ‚• = state_index(mdp, state)
                    aâ‚• = action_index(mdp, trajectory.action_hist[h])

                    dl_dÎ¸â‚– = Î² * ( dQâ‚–[sâ‚•,aâ‚•] - sum( [ state_action_lh(Ï€áµ¦,sâ‚•,aiâ») * dQâ‚–[sâ‚•,aiâ»] for aiâ» in actions_i ] ) )
                    âˆ‡ğ“›[k] += dl_dÎ¸â‚–
                end
            end
        end
        Î¸ += Î± * âˆ‡ğ“›
        # Î¸.values /= maximum(Î¸.values)
        elapsed = toq()

        verbose ? println("Iteration took $elapsed seconds") : nothing
    end

    # Log EVD
    if verbose && ground_policy !== nothing
        # Need to change this to account for features
        Ï€áµ£ = solve_mdp(mdp, Î¸)
        váµ£ = policy_evaluation(mdp, Ï€áµ£)
        push!(EVD, norm(v-váµ£))
        println("Final EVD: $(EVD[end])")
    end

    Î¸, EVD
end



# i2s âœ“
# Ï€2transition âœ“
