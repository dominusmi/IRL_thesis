import Base.+, Base.-, Base.copy

abstract type RewardFunction end

mutable struct DPMBIRLReward <: RewardFunction
    weights::Array{<:Number}
    Ï€::Policy
    Ï€áµ¦::Array{<:AbstractFloat,2}
    invT::Array{<:AbstractFloat,2}
    ð“›::Float64
    âˆ‡ð“›::Array{<:AbstractFloat,1}
    values::Array{<:Number}
    DPMBIRLReward(weights::Array{<:Number}) = new(weights)
    DPMBIRLReward(weights::Array{<:Number},
        Ï€::Policy,
        Ï€áµ¦::Array{<:AbstractFloat,2},
        invT::Array{<:AbstractFloat,2},
        ð“›::Float64,
        âˆ‡ð“›::Array{<:AbstractFloat,1}) = new(weights, Ï€, Ï€áµ¦, invT, ð“›, âˆ‡ð“›)
    DPMBIRLReward(weights::Array{<:Number},
        Ï€::Policy, Ï€áµ¦::Array{<:AbstractFloat,2},
        invT::Array{<:AbstractFloat,2}, ð“›::Float64,
        âˆ‡ð“›::Array{<:AbstractFloat,1}, v::Vector{<:Number}) = new(weights, Ï€, Ï€áµ¦, invT, ð“›, âˆ‡ð“›, v)
end


function copy(r::DPMBIRLReward)
    DPMBIRLReward(copy(r.weights), DiscreteValueIteration.Policy(r.Ï€),
                    copy(r.Ï€áµ¦), copy(r.invT), copy(r.ð“›), copy(r.âˆ‡ð“›), copy(r.values))
end

function +(r::RewardFunction, values::Array{<:AbstractFloat})

    typeof(r)(r.weights+values)
end

function -(r::RewardFunction, values::Array{<:AbstractFloat})
    typeof(r)(r.weights-values)
end

""" Get weights """
r2weights(r::RewardFunction) = r.weights


"""
    Sample a new reward value for every state from gaussian
"""
function sample(T::Type{<:RewardFunction}, features)
    # Choi states that he sets 80% of reward values to zero
    weights = rand(Normal(0,1), features)
    # values = [ rand()<0.2?value:0.0 for value in values]
    T(weights)
end

"""
    Calculates proposal value of new reward
"""
function proposal_distribution(râ‚::RewardFunction, râ‚‚::RewardFunction, âˆ‡logTarget::Array, Ï„)
    # D = size(râ‚.values,1)
    g = râ‚‚.weights - râ‚.weights - 0.25*Ï„^2 * âˆ‡logTarget
    g = -inv(-2*Ï„^2) * norm(g)^2
    # This is the correct calculation, but the initial constant cancels out
    # g = inv( (2*Ï€*Ï„^2)^(D/2) ) * exp(g)
    g
end


function update_reward!(Î¸::RewardFunction, mdp, Ï‡â‚–, glb::Globals)
    global globals
    # Update real space from features
    Î¸.values = glb.Ï• * Î¸.weights
    # Solve mdp with current reward
    Î¸.Ï€  = solve_mdp(mdp, Î¸)
    # Find Boltzmann policy
    Î¸.Ï€áµ¦ = calÏ€áµ¦(mdp, Î¸.Ï€.qmat, glb)
    # Prepare variables for gradient
    Î¸.invT = calInvTransition(mdp, Î¸.Ï€áµ¦, glb.Î³)
    # Calculates value and gradient of trajectory likelihood
    Î¸.ð“› = calð“›(mdp, Î¸.Ï€áµ¦, Ï‡â‚–, glb)
    Î¸.âˆ‡ð“› = calâˆ‡ð“›(mdp, Î¸.invT, Î¸.Ï€áµ¦, Ï‡â‚–, glb)
end

"""
    Return normal prior of reward and its gradient
"""
function log_prior(r::RewardFunction)
    # variance = ÏƒÂ²
    var = 1.0
    sum(-(r.weights'*r.weights)./(2*var^2))+log(inv(sqrt(2*3.1415*var))), -r.weights ./ var
end


function values(r::RewardFunction, Ï•)
    Ï•*r.weights
end

function rewards_covariance!(Ïƒ, _log)
    n_steps = size(_log[:rewards],1)
    n_features = size(_log[:rewards][1],1)
    rewards = zeros(n_steps, n_features)
    for i in 1:n_steps
        rewards[i,:] = _log[:rewards][i][1].values
    end
    Ïƒ = Ïƒ .* [sqrt(cov(rewards[rewards[:,1].!==0.0,i])) for i in 1:n_features]
    Ïƒ
end


function compute_potential_reward(Î¸::RewardFunction, mdp::MDP, update::Symbol, Ï‡â‚–::Array{MDPHistory}, Ïƒ::Matrix{<:AbstractFloat}, Ï„::AbstractFloat, Î±::AbstractFloat, glb::Globals)
    if update == :langevin_rand
        Ïµ = rand(Normal(0,1), glb.n_features)
        indeces = rand(glb.n_features) .< 0.2
        Ïµ[indeces] = 0.0
        Î¸â» = Î¸ + Î±*Î¸.âˆ‡ð“› + Ï„*Ïµ
        Î¸â».weights ./= sum(abs.(Î¸â».weights))
    elseif update == :MH
        # Ïµ = rand(Normal(0,1), n_features)
        Ïµ = rand(MultivariateNormal(Ïƒ))
        Î¸â» = Î¸ + Ïµ
    else
        Î¸â» = Î¸ + Î±*Î¸.âˆ‡ð“›
        Î¸â».weights ./= sum(abs.(Î¸â».weights))
    end
    Î¸â».values = values(Î¸â», glb.Ï•)

    # Solve everything for potential new reward
    Î¸â».Ï€  = solve_mdp(mdp, Î¸â»)
    Î¸â».Ï€áµ¦ = calÏ€áµ¦(mdp, Î¸â».Ï€.qmat, glb)
    Î¸â».ð“› = calð“›(mdp, Î¸â».Ï€áµ¦, Ï‡â‚–, glb)

    if update !== :MH
        Î¸â».invT = calInvTransition(mdp, Î¸â».Ï€áµ¦, glb.Î³)
        Î¸â».âˆ‡ð“› = calâˆ‡ð“›(mdp, Î¸â».invT, Î¸â».Ï€áµ¦,  Ï‡â‚–, glb)
    end
    Î¸â»
end
