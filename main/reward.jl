import Base.+, Base.-, Base.copy

mutable struct RewardFunction
    values::Array{<:Number}
    Ï€::Policy
    Ï€áµ¦::Array{<:AbstractFloat,2}
    invT::Array{<:AbstractFloat,2}
    ð“›::Float64
    âˆ‡ð“›::Array{<:AbstractFloat,1}
    RewardFunction(values::Array{<:Number}) = new(values)
end


function copy(r::RewardFunction)
    RewardFunction(copy(r.values), DiscreteValueIteration.Policy(r.Ï€),
                    copy(r.Ï€áµ¦), copy(r.invT), copy(ð“›), copy(âˆ‡ð“›))
end

function +(r::RewardFunction, values::Array{<:AbstractFloat})
    RewardFunction(r.values+values)
end

function -(r::RewardFunction, values::Array{<:AbstractFloat})
    RewardFunction(r.values-values)
end

"""
    Sample a new reward value for every state from gaussian
"""
function sample(::Type{RewardFunction}, features)
    # Choi states that he sets 80% of reward values to zero
    values = rand(Normal(0,1), features)
    # values = [ rand()<0.2?value:0.0 for value in values]
    RewardFunction(values)
end

"""
    Calculates proposal value of new reward
"""
function proposal_distribution(râ‚::RewardFunction, râ‚‚::RewardFunction, âˆ‡logTarget::Array, Ï„)
    D = size(râ‚.values,1)
    g = râ‚‚.values - râ‚.values - 0.25*Ï„^2 * âˆ‡logTarget
    g = inv(-2*Ï„^2) * norm(g)^2
    # This is the correct calculation, but the initial constant cancels out
    # g = inv( (2*Ï€*Ï„^2)^(D/2) ) * exp(g)
    g
end


function update_reward!(Î¸::RewardFunction, mdp, Ï‡â‚–, glb::Globals)
    global globals
    # Solve mdp with current reward
    Î¸.Ï€  = solve_mdp(mdp, Î¸)
    # Find Boltzmann policy
    Î¸.Ï€áµ¦ = calÏ€áµ¦(mdp, Î¸.Ï€.qmat, glb)

    # Prepare variables for gradient
    Î¸.invT = calInvTransition(mdp, Î¸.Ï€áµ¦, glb.Î³)
    # Calculates value and gradient of trajectory likelihood
    Î¸.ð“› = calð“›(mdp, Î¸.Ï€áµ¦, Ï‡â‚–, glb)
    Î¸.âˆ‡ð“› = calâˆ‡ð“›(mdp, Î¸.invT, Î¸.Ï€áµ¦, Ï‡â‚–, glb)
end

"""
    Return normal prior of reward and its gradient
"""
function log_prior(r::RewardFunction)
    # variance = ÏƒÂ²
    var = 1.0
    sum(-(r.values'*r.values)./(2*var)), -r.values ./ var
end
