include("reward.jl")
include("utilities/gridworld.jl")
include("utilities/policy.jl")


function calQ·µ£(mdp, r, Œ≤ = 0.5)
    states = ordered_states(mdp)

    Q = zeros(size(states,1), size(actions(mdp),1))
    policy = solve_mdp(mdp, r)
    Œ≥ = mdp.discount_factor

    for s in states
        si = state_index(s)
        for a in actions(mdp)
            ai = action_index(a)
            states‚Åª, p = transitions
            _sumS = 0.

            for (j,s‚Åª) in enumerate(states‚Åª)
                si‚Åª = state_index(s‚Åª)
                _sumQ = 0
                for a‚Åª in actions(mdp, s‚Åª)
                    _softmax = exp( Œ≤* policy.qmat[si‚Åª,action_index(a‚Åª)]) / sum(Œ≤* exp.(policy.qmat[si‚Åª,:]))
                    _sumQ += policy.qmat[si‚Åª,action_index(a‚Åª)] * _softmax
                end
                _sumS += p[j] * _sumQ
            end

            Q[si,ai] = reward(mdp, s, a) + Œ≥ * _sumS
        end
    end
    policy, Q
end


function calœÄ·µ¶(mdp, Q, Œ≤=0.5)
    states = ordered_states(mdp)
    œÄ·µ£ = zeros(size(states,1)-1, size(actions(mdp),1))

    for s in states[1:end-1]
        si = state_index(mdp,s)
        softmax_denom = sum(exp.(Œ≤*Q[si,:]))
        for a in actions(mdp)
            ai = action_index(mdp,a)
            œÄ·µ£[si,ai] = exp(Œ≤*Q[si,ai]) / softmax_denom
        end
    end
    œÄ·µ£
end

function œÄ2transition(mdp, œÄ)
    n_states = size(œÄ,1)
    T = zeros(n_states, n_states)

    for s in 1:n_states
        nbs = state_neighbours(mdp, s)
        for (i,nb) in enumerate(nbs)
            T[s,nb] = œÄ[s,i]
        end
    end
    T
end

"""
    Given an action, generates the SxS probability transition matrix P‚Çê
"""

function a2transition(mdp, a)
    states = ordered_states(mdp)
    n_states = size(states,1)-1
    P‚Çê = zeros(n_states, n_states)

    for s in states[1:end-1]
        si = state_index(mdp,s)
        states‚Åª = transition(mdp, s, a)
        states‚Åª, p = states‚Åª.vals, states‚Åª.probs
        for (j,s‚Åª) in enumerate(states‚Åª)
            if isterminal(mdp, s‚Åª)
                continue
            end
            s‚Åª = POMDPModels.inbounds(mdp, s‚Åª) ? s‚Åª : s
            si‚Åª = state_index(mdp, s‚Åª)
            P‚Çê[si, si‚Åª] = p[j]
        end
    end
    P‚Çê
end

"""
    Calculates inv(I-Œ≥P‚Çö·µ¢), where P‚Çö·µ¢ is the matrix of transition probabilities
    of size SxS
"""
function calInvTransition(mdp, œÄ·µ¶, Œ≥)
    n_states = size(œÄ·µ¶,1)
    I = eye(n_states)
    P‚Çö·µ¢ = œÄ2transition(mdp, œÄ·µ¶)
    inv( I-Œ≥*P‚Çö·µ¢ )
end

"""
    Currently unused
"""
function caldQ‚Çñ!(dQ‚Çñ, mdp, œï, invT, P‚Çê, œÄ·µ¶, k)
    states = ordered_states(mdp)
    œÄ‚Çê = zeros(size(states,1)-1)

    # œÄ "marginal"
    for s in states[1:end-1]
        si      = state_index(mdp, s)
        œÄ‚Çê[si]  = sum( œÄ·µ¶[si,:] ) * œï[si,k]
    end

    # dQ for each action
    for a in actions(mdp)
        ai = action_index(mdp,a)
        dQ‚Çñ[:,ai] = œï[:,k] + mdp.discount_factor * P‚Çê[ai] * invT * œÄ‚Çê
    end
end


"""
    (proportional) Likelihood function for a single state action
    Normally should have normalisiation, but not important when calculating ‚àáùìõ
"""
state_action_lh(œÄ·µ¶, s,a) = œÄ·µ¶[s,a]


"""
    Calculates the log likelihood given a Q-value
"""
function log_likelihood(mdp::GridWorld, Q::Array{<:AbstractFloat,2}, trajectories::Array{<:MDPHistory})
    llh = 0.
    BoltzmannQ = Q .- log.(sum(exp.(Q),2))

    for (i,trajectory) in enumerate(trajectories)
        normalising = size(trajectory.state_hist,1)-1
        for (i,state) in enumerate(trajectory.state_hist[1:end-1])
            s = state_index(mdp, state)
            a = action_index(mdp, trajectory.action_hist[i])
            llh += BoltzmannQ[s,a] / normalising
        end
    end
    llh
end



"""
    Executes a maximum likelihood inverse reinforcement learning
    mdp:            the problem
    œï:              operator to features space
    trajectories:   experts' trajectories
    iterations:     number of iterations to run for
    Œ±:              learning rate
    Œ≤:              confidence parameter
"""
function MLIRL(mdp, œï, trajectories, iterations; Œ±=0.1, Œ≤=0.5, ground_policy = nothing, verbose=true)

    srand(1)
    Œ∏ = sample(RewardFunction, size(œï,2))
    Œ≥ = mdp.discount_factor
    states = ordered_states(mdp)

    n_states  = size(states,1)-1
    n_actions = size( actions(mdp),1 )
    n_features = size(œï,2)

    EVD = []

    if verbose && ground_policy !== nothing
        v = policy_evaluation(mdp, ground_policy)
    end

    for t in 1:iterations
        tic()
        # œÄ, Q·µ£   = calQ·µ£(mdp, Œ∏)
        œÄ  = solve_mdp(mdp, Œ∏)
        œÄ·µ¶ = calœÄ·µ¶(mdp, œÄ.qmat, Œ≤)

        # Log EVD
        if verbose && ground_policy !== nothing
            tic()
            # Need to change this to account for features
            œÄ·µ£ = solve_mdp(mdp, Œ∏)
            v·µ£ = policy_evaluation(mdp, œÄ·µ£)
            push!(EVD, norm(v-v·µ£))
            elapsed = toq()
        end


        invT = calInvTransition(mdp, œÄ·µ¶, Œ≥)
        actions_i = action_index.(mdp, actions(mdp))
        dQ = zeros(n_states, n_actions)
        ‚àáùìõ = SharedArray{Float64,1}(zeros(n_features))

        # Precpmputes transition matrix for all actions
        # (independent of features)
        P‚Çê = a2transition.(mdp,actions(mdp))


        # Calculates gradient per feature
        for k in 1:n_features
            dQ‚Çñ = zeros( n_states, n_actions )
            caldQ‚Çñ!(dQ‚Çñ, mdp, œï, invT, P‚Çê, œÄ·µ¶, k)

            # dQ = zeros( n_states, n_actions )
            # œÄ‚Çê = zeros(size(states,1)-1)
            #
            # # œÄ "marginal"
            # for s in states[1:end-1]
            #     si      = state_index(mdp, s)
            #     œÄ‚Çê[si]  = sum( œÄ·µ¶[si,:] ) * œï[si,k]
            # end
            #
            # # dQ for each action
            # for a in actions(mdp)
            #     ai = action_index(mdp,a)
            #     dQ[:,ai] = œï[:,k] + Œ≥ * P‚Çê[ai] * invT * œÄ‚Çê
            # end
            #
            # k==1 ? println(norm(dQ-dQ‚Çñ)) : nothing

            # Calculates total gradient over trajectories
            for (m,trajectory) in enumerate(trajectories)
                for (h,state) in enumerate(trajectory.state_hist[1:end-1])
                    s‚Çï = state_index(mdp, state)
                    a‚Çï = action_index(mdp, trajectory.action_hist[h])

                    dl_dŒ∏‚Çñ = Œ≤ * ( dQ‚Çñ[s‚Çï,a‚Çï] - sum( [ state_action_lh(œÄ·µ¶,s‚Çï,ai‚Åª) * dQ‚Çñ[s‚Çï,ai‚Åª] for ai‚Åª in actions_i ] ) )
                    ‚àáùìõ[k] += dl_dŒ∏‚Çñ
                end
            end
        end
        Œ∏ += Œ± * ‚àáùìõ
        # Œ∏.values /= maximum(Œ∏.values)
        elapsed = toq()

        verbose ? println("Iteration took $elapsed seconds") : nothing
    end

    # Log EVD
    if verbose && ground_policy !== nothing
        # Need to change this to account for features
        œÄ·µ£ = solve_mdp(mdp, Œ∏)
        v·µ£ = policy_evaluation(mdp, œÄ·µ£)
        push!(EVD, norm(v-v·µ£))
        println("Final EVD: $(EVD[end])")
    end

    Œ∏, EVD
end



# i2s ‚úì
# œÄ2transition ‚úì
