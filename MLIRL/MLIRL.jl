include("reward.jl")
include("utilities/gridworld.jl")
include("utilities/policy.jl")

function calQ·µ£(mdp, r, Œ≤ = 0.5)
    states = ordered_states(mdp)

    Q = zeros(size(states,1), size(actions(mdp),1))
    policy = solve_mdp(mdp, r)
    Œ≥ = mdp.discount_factor

    for s in states
        si = state_index(s)
        for a in actions(mdp)
            ai = action_index(a)
            states‚Åª, p = transitions
            _sumS = 0.

            for (j,s‚Åª) in enumerate(states‚Åª)
                si‚Åª = state_index(s‚Åª)
                _sumQ = 0
                for a‚Åª in actions(mdp, s‚Åª)
                    _softmax = exp( Œ≤* policy.qmat[si‚Åª,action_index(a‚Åª)]) / sum(Œ≤* exp.(policy.qmat[si‚Åª,:]))
                    _sumQ += policy.qmat[si‚Åª,action_index(a‚Åª)] * _softmax
                end
                _sumS += p[j] * _sumQ
            end

            Q[si,ai] = reward(mdp, s, a) + Œ≥ * _sumS
        end
    end
    policy, Q
end


function calœÄ·µ¶(mdp, Q, Œ≤=0.5)
    states = ordered_states(mdp)
    œÄ·µ£ = zeros(size(states,1)-1, size(actions(mdp),1))

    for s in states[1:end-1]
        si = state_index(mdp,s)
        softmax_denom = sum(exp.(Œ≤*Q[si,:]))
        for a in actions(mdp)
            ai = action_index(mdp,a)
            œÄ·µ£[si,ai] = exp(Œ≤*Q[si,ai]) / softmax_denom
        end
    end
    œÄ·µ£
end

function œÄ2transition(mdp, œÄ)
    n_states = size(œÄ,1)
    T = zeros(n_states, n_states)

    for s in 1:n_states
        nbs = state_neighbours(mdp, s)
        for (i,nb) in enumerate(nbs)
            T[s,nb] = œÄ[s,i]
        end
    end
    T
end

"""
    Given an action, generates the SxS probability transition matrix P‚Çê
"""
function a2transition(mdp, a)
    states = ordered_states(mdp)
    n_states = size(states,1)-1
    P‚Çê = zeros(n_states, n_states)

    for s in states[1:end-1]
        si = state_index(mdp,s)
        states‚Åª = transition(mdp, s, a)
        states‚Åª, p = states‚Åª.vals, states‚Åª.probs
        for (j,s‚Åª) in enumerate(states‚Åª)
            if isterminal(mdp, s‚Åª)
                continue
            end
            s‚Åª = POMDPModels.inbounds(mdp, s‚Åª) ? s‚Åª : s
            si‚Åª = state_index(mdp, s‚Åª)
            P‚Çê[si, si‚Åª] = p[j]
        end
    end
    P‚Çê
end


function calInvTransition(mdp, œÄ·µ¶, Œ≥)
    n_states = size(œÄ·µ¶,1)
    I = eye(n_states)
    P‚Çö·µ¢ = œÄ2transition(mdp, œÄ·µ¶)
    inv( I-Œ≥*P‚Çö·µ¢ )
end

function MLIRL(mdp, œï, trajectories, iterations, learning_rate; ground_policy = nothing)
    srand(1)
    Œ∏ = sample(RewardFunction, size(œï,2))
    Œ≥ = mdp.discount_factor
    states = ordered_states(mdp)

    n_states  = size(states,1)-1
    n_actions = size( actions(mdp),1 )
    n_features = size(œï,2)

    EVD = []

    if ground_policy !== nothing
        v = policy_evaluation(mdp, ground_policy)
    end


    Œ≤ = 0.5
    for t in 1:iterations
        # œÄ, Q·µ£   = calQ·µ£(mdp, Œ∏)
        œÄ  = solve_mdp(mdp, Œ∏)
        œÄ·µ¶ = calœÄ·µ¶(mdp, œÄ.qmat, Œ≤)

        # Log EVD
        if ground_policy !== nothing
            # Need to change this to account for features
            œÄ·µ£ = solve_mdp(mdp, Œ∏)
            v·µ£ = policy_evaluation(mdp, œÄ·µ£)
            push!(EVD, norm(v-v·µ£))
        end


        invT = calInvTransition(mdp, œÄ·µ¶, Œ≥)
        actions_i = action_index.(mdp, actions(mdp))
        dQ = zeros(n_states, n_actions)

        ‚àáùìõ = zeros(n_features)

        for k in 1:n_features

            dQ = zeros( n_states, n_actions )
            œÄ‚Çê = zeros(size(states,1)-1)

            # œÄ "marginal"
            for s in states[1:end-1]
                si      = state_index(mdp, s)
                œÄ‚Çê[si]  = sum( œÄ·µ¶[si,:] ) * œï[si,k]
            end

            # dQ for each action
            for a in actions(mdp)
                P‚Çê = a2transition(mdp, a)
                ai = action_index(mdp,a)
                dQ[:,ai] = œï[:,k] + Œ≥ * P‚Çê * invT * œÄ‚Çê
            end

            for (m,trajectory) in enumerate(trajectories)
                for (h,state) in enumerate(trajectory.state_hist[1:end-1])
                    s‚Çï = state_index(mdp, state)
                    a‚Çï = action_index(mdp, trajectory.action_hist[h])

                    # let s‚Çï, a‚Çï be current state, action
                    dl_dŒ∏‚Çñ = Œ≤ * ( dQ[s‚Çï,a‚Çï] - sum( [ lh(œÄ·µ¶,s‚Çï,ai‚Åª) * dQ[s‚Çï,ai‚Åª] for ai‚Åª in actions_i ] ) )
                    ‚àáùìõ[k] += dl_dŒ∏‚Çñ
                end
            end
        end
        Œ∏ += ‚àáùìõ
        # ‚àáQ = cal‚àáQ(œï)
    end
    EVD
end

lh(œÄ·µ¶, s,a) = œÄ·µ¶[s,a]

mdp, policy = generate_gridworld(10,10,Œ≥=0.9)
trajectories = generate_trajectories(mdp, policy, 50)
œï = eye(100)
œÑ = 0.1

transition(mdp, GridWorldState(1,1), :up)

P‚Çê = a2transition(mdp, :up)

state_index(mdp, GridWorldState(4,2))

EVD = MLIRL(mdp, œï, trajectories, 50, œÑ; ground_policy = policy)

using Plots
Plots.plot(EVD)

# i2s ‚úì
# œÄ2transition ‚úì
